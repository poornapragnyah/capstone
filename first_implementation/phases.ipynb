{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed3705b-39d8-4557-b8b9-7b4b2b958c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853fa2d-7151-4c8d-a7b3-0098283c0ab5",
   "metadata": {},
   "source": [
    "# Phase 0: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d00cbab-a58d-449f-89f6-46329d47b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGTextH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to load data from an HDF5 file.\n",
    "\n",
    "    Args:\n",
    "        h5_path (str): Path to the HDF5 file.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path):\n",
    "        self.h5_path = h5_path\n",
    "        self.h5_file = None  # File handle will be opened in __getitem__ for multiprocessing\n",
    "        \n",
    "        # Open the file once to get the length\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            self.eeg_data_shape = f['eeg'].shape\n",
    "            self.n_samples = self.eeg_data_shape[0]\n",
    "            # Assuming 'text_tokens' is a variable-length dataset\n",
    "            self.text_tokens_handle = f['input_ids'] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open the file here if not already open. This is better for DataLoader with num_workers > 0.\n",
    "        if self.h5_file is None:\n",
    "            self.h5_file = h5py.File(self.h5_path, 'r')\n",
    "            \n",
    "        # Retrieve the EEG data for the given index\n",
    "        eeg_sample = self.h5_file['eeg'][idx, :, :]\n",
    "        \n",
    "        # Retrieve the variable-length text tokens for the given index\n",
    "        text_sample = self.h5_file['input_ids'][idx]\n",
    "\n",
    "        # Convert numpy arrays from h5py to PyTorch tensors\n",
    "        eeg_tensor = torch.from_numpy(eeg_sample.astype('float32'))\n",
    "        text_tensor = torch.from_numpy(text_sample.astype('int64'))\n",
    "        \n",
    "        return eeg_tensor, text_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a672b2-d6bd-4067-b9c7-b7a19137a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset with 28000 samples.\n",
      "Sample 0 EEG shape: torch.Size([62, 400])\n",
      "Sample 0 Text shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- How to use it ---\n",
    "# 1. Define the path to your data file\n",
    "H5_FILE_PATH = \"/home/poorna/data/eeg_dataset.h5\" \n",
    "\n",
    "# 2. Instantiate the dataset\n",
    "# This 'dataset' object is now ready for the next phases.\n",
    "dataset = EEGTextH5Dataset(H5_FILE_PATH)\n",
    "\n",
    "print(f\"Successfully loaded dataset with {len(dataset)} samples.\")\n",
    "eeg, text = dataset[0] # Test it\n",
    "print(\"Sample 0 EEG shape:\", eeg.shape)\n",
    "print(\"Sample 0 Text shape:\", text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbf074-e92c-4f22-8b90-6104e4715759",
   "metadata": {},
   "source": [
    "# Phase 1: Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd3b069-00be-48b5-8e47-5a450d3bd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 28000\n",
      "Split sizes -> Train: 22400, Validation: 2800, Test: 2800\n"
     ]
    }
   ],
   "source": [
    "# Define the proportions for the split\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT = 0.8, 0.1, 0.1\n",
    "\n",
    "# Get the total number of samples\n",
    "N = len(dataset) # This will be 28000\n",
    "\n",
    "# Calculate the actual number of samples for each split\n",
    "n_train = int(N * TRAIN_PCT)\n",
    "n_val   = int(N * VAL_PCT)\n",
    "n_test  = N - n_train - n_val\n",
    "\n",
    "# Use a fixed generator seed for a reproducible split\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Perform the split\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=g)\n",
    "\n",
    "print(f\"Total samples: {N}\")\n",
    "print(f\"Split sizes -> Train: {len(train_ds)}, Validation: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ecc43c-ffdb-4a34-bbc7-b49f7bce987b",
   "metadata": {},
   "source": [
    "# Phase 2: Custom collate_fn for Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d02797-fd93-4b69-923a-c47665359ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`collate_batch` function defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "# IMPORTANT: Adjust these token IDs to match your specific tokenizer.\n",
    "# We'll assume '0' is the padding token for now.\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1  # Start Of Sentence\n",
    "EOS_ID = 2  # End Of Sentence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Collates a list of samples into a padded batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of tuples, where each tuple is (eeg_tensor, text_ids_tensor).\n",
    "                      e.g., [ (torch.Size([62, 400]), torch.Size([64])), \n",
    "                              (torch.Size([62, 400]), torch.Size([58])), ... ]\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - eeg_batch (torch.Tensor): A tensor of EEG signals, shape [B, 62, 400].\n",
    "            - text_padded (torch.Tensor): A padded tensor of text IDs, shape [B, T_max].\n",
    "            - text_lengths (torch.Tensor): A tensor of original text lengths, shape [B].\n",
    "    \"\"\"\n",
    "    eeg_list, text_list = [], [import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "# IMPORTANT: Adjust these token IDs to match your specific tokenizer.\n",
    "# We'll assume '0' is the padding token for now.\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1  # Start Of Sentence\n",
    "EOS_ID = 2  # End Of Sentence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Collates a list of samples into a padded batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of tuples, where each tuple is (eeg_tensor, text_ids_tensor).\n",
    "                      e.g., [ (torch.Size([62, 400]), torch.Size([64])), \n",
    "                              (torch.Size([62, 400]), torch.Size([58])), ... ]\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - eeg_batch (torch.Tensor): A tensor of EEG signals, shape [B, 62, 400].\n",
    "            - text_padded (torch.Tensor): A padded tensor of text IDs, shape [B, T_max].\n",
    "            - text_lengths (torch.Tensor): A tensor of original text lengths, shape [B].\n",
    "    \"\"\"\n",
    "    eeg_list, text_list = [], []\n",
    "    for eeg, txt in batch:\n",
    "        eeg_list.append(eeg)\n",
    "        text_list.append(txt.long()) # Ensure text IDs are LongTensors\n",
    "\n",
    "    # Stack EEG signals (they are all the same size)\n",
    "    eeg_batch = torch.stack(eeg_list, dim=0)\n",
    "\n",
    "    # Store the original, un-padded lengths of each text sequence\n",
    "    text_lengths = torch.tensor([t.numel() for t in text_list], dtype=torch.long)\n",
    "\n",
    "    # Pad the text sequences to the length of the longest sequence in the batch\n",
    "    # `batch_first=True` makes the output shape [Batch, SequenceLength]\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=PAD_ID)\n",
    "\n",
    "    return eeg_batch.float(), text_padded, text_lengths\n",
    "\n",
    "print(\"`collate_batch` function defined successfully.\")]\n",
    "    for eeg, txt in batch:\n",
    "        eeg_list.append(eeg)\n",
    "        text_list.append(txt.long()) # Ensure text IDs are LongTensors\n",
    "\n",
    "    # Stack EEG signals (they are all the same size)\n",
    "    eeg_batch = torch.stack(eeg_list, dim=0)\n",
    "\n",
    "    # Store the original, un-padded lengths of each text sequence\n",
    "    text_lengths = torch.tensor([t.numel() for t in text_list], dtype=torch.long)\n",
    "\n",
    "    # Pad the text sequences to the length of the longest sequence in the batch\n",
    "    # `batch_first=True` makes the output shape [Batch, SequenceLength]\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=PAD_ID)\n",
    "\n",
    "    return eeg_batch.float(), text_padded, text_lengths\n",
    "\n",
    "print(\"`collate_batch` function defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8536c-37d2-4b4f-8913-4e86a0238a54",
   "metadata": {},
   "source": [
    "# Phase 3: DataLoader Instantiation and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a462032-05b0-48e6-8ffc-e716d5c58646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully.\n",
      "\n",
      "--- Verification Step ---\n",
      "Fetching one batch from the train_loader to inspect its shape...\n",
      "\n",
      "EEG batch shape: torch.Size([32, 62, 400])\n",
      "Padded Text batch shape: torch.Size([32, 64])\n",
      "Text Lengths batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "# Hyperparameters for the DataLoader\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2 # Adjust this based on your machine's capability (0, 2, or 4 are common)\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "# - `shuffle=True` is important for training to ensure the model sees data in a random order each epoch.\n",
    "# - `collate_fn=collate_batch` tells the DataLoader to use our custom padding function.\n",
    "train_loader = DataLoader(train_ds, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, \n",
    "                          pin_memory=True, \n",
    "                          collate_fn=collate_batch)\n",
    "\n",
    "# Create the DataLoader for the validation set\n",
    "# - `shuffle=False` is used for validation/testing for consistent evaluation.\n",
    "val_loader = DataLoader(val_ds, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, \n",
    "                        pin_memory=True, \n",
    "                        collate_fn=collate_batch)\n",
    "\n",
    "print(\"DataLoaders created successfully.\")\n",
    "print(\"\\n--- Verification Step ---\")\n",
    "print(\"Fetching one batch from the train_loader to inspect its shape...\")\n",
    "\n",
    "# Retrieve a single batch of data\n",
    "eeg_batch, text_padded_batch, text_lengths_batch = next(iter(train_loader))\n",
    "\n",
    "# Print the shapes of the tensors in the batch\n",
    "print(\"\\nEEG batch shape:\", eeg_batch.shape)\n",
    "print(\"Padded Text batch shape:\", text_padded_batch.shape)\n",
    "print(\"Text Lengths batch shape:\", text_lengths_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263d3f3-1a29-4470-b300-45f835e51693",
   "metadata": {},
   "source": [
    "# Phase 4: Model Architecture (Encoderâ€“Decoder with Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d88f327a-b8b6-4ec5-a31d-72e98a564fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the EEG signal from [B, 62, 400] into a sequence of feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=62, conv_dim=128, enc_hidden=256, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # 1D Convolutions to extract local and spatial features\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(128, conv_dim, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(conv_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        # Bidirectional GRU to capture long-range temporal dependencies\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=conv_dim,\n",
    "            hidden_size=enc_hidden,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=False # We will permute input to [T, B, C]\n",
    "        )\n",
    "\n",
    "    def forward(self, eeg):\n",
    "        # eeg shape: [B, C, T] -> [B, 62, 400]\n",
    "        x = self.conv(eeg)          # -> [B, conv_dim, T]\n",
    "        x = x.permute(2, 0, 1)      # -> [T, B, conv_dim] for RNN\n",
    "        enc_outputs, enc_hidden = self.rnn(x)\n",
    "        # enc_outputs: [T, B, 2 * H] (all hidden states)\n",
    "        # enc_hidden: [2 * num_layers, B, H] (final hidden state)\n",
    "        return enc_outputs, enc_hidden\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    \"\"\" Implements Luong's general dot-product attention. \"\"\"\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        # Linear layer to project encoder outputs to the same dimension as the decoder hidden state\n",
    "        self.attn = nn.Linear(enc_dim, dec_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden shape: [1, B, dec_dim]\n",
    "        # encoder_outputs shape: [T_enc, B, enc_dim]\n",
    "        \n",
    "        # Calculate alignment scores\n",
    "        # (B, T_enc, dec_dim) @ (B, dec_dim, 1) -> (B, T_enc, 1)\n",
    "        scores = torch.bmm(self.attn(encoder_outputs).permute(1, 0, 2), \n",
    "                           decoder_hidden.permute(1, 2, 0))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=1) # -> [B, T_enc, 1]\n",
    "        \n",
    "        # Calculate context vector (weighted sum of encoder outputs)\n",
    "        # (B, 1, T_enc) @ (B, T_enc, enc_dim) -> (B, 1, enc_dim)\n",
    "        context = torch.bmm(attn_weights.permute(0, 2, 1), encoder_outputs.permute(1, 0, 2))\n",
    "        \n",
    "        return context, attn_weights.squeeze(-1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes the encoder's output into a sequence of text tokens.\n",
    "    (CORRECTED VERSION)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim=256, enc_hidden=256, dec_hidden=256, num_layers=2, \n",
    "                 pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        \n",
    "        enc_dim = enc_hidden * 2 # From bidirectional encoder\n",
    "        self.attention = LuongAttention(enc_dim, dec_hidden)\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=emb_dim + enc_dim,\n",
    "            hidden_size=dec_hidden,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.fc_out = nn.Linear(dec_hidden, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Bridge layer expects a concatenated hidden state of size 2 * enc_hidden\n",
    "        self.bridge = nn.Linear(enc_dim, dec_hidden)\n",
    "\n",
    "    def init_hidden(self, enc_hidden):\n",
    "        # enc_hidden shape: [2 * num_layers, B, H]\n",
    "        \n",
    "        # --- START OF FIX ---\n",
    "        # Separate the forward (even indices) and backward (odd indices) hidden states\n",
    "        forward_h = enc_hidden[0::2]\n",
    "        backward_h = enc_hidden[1::2]\n",
    "        \n",
    "        # Concatenate them along the feature dimension (dim=2)\n",
    "        # This correctly creates a tensor of shape [num_layers, B, 2 * H]\n",
    "        enc_hidden_cat = torch.cat([forward_h, backward_h], dim=2)\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        # Pass the properly shaped tensor through the bridge layer\n",
    "        return torch.tanh(self.bridge(enc_hidden_cat))\n",
    "\n",
    "    def forward(self, token, decoder_hidden, encoder_outputs):\n",
    "        # token shape: [B]\n",
    "        # decoder_hidden shape: [num_layers, B, dec_hidden]\n",
    "        # encoder_outputs shape: [T_enc, B, enc_dim]\n",
    "        token = token.unsqueeze(0) # -> [1, B] for embedding\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(token)) # -> [1, B, emb_dim]\n",
    "        \n",
    "        context, attn_weights = self.attention(decoder_hidden[-1].unsqueeze(0), encoder_outputs) # Use top layer hidden state for attention\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, context.permute(1,0,2)), dim=2) # -> [1, B, emb_dim + enc_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, decoder_hidden)\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0)) # -> [B, vocab_size]\n",
    "        \n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper class to combine the Encoder and Decoder.\n",
    "    (CORRECTED VERSION 2)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, enc_hidden=256, dec_hidden=256, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = EEGEncoder(enc_hidden=enc_hidden, dropout=dropout)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, enc_hidden=enc_hidden, dec_hidden=dec_hidden, \n",
    "                               pad_id=pad_id, dropout=dropout)\n",
    "\n",
    "    def forward(self, eeg, target_text, teacher_forcing_ratio=0.5):\n",
    "        # eeg: [B, 62, 400], target_text: [B, T_out]\n",
    "        batch_size = eeg.shape[0]\n",
    "        target_len = target_text.shape[1]\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(eeg)\n",
    "        decoder_hidden = self.decoder.init_hidden(encoder_hidden)\n",
    "        \n",
    "        # First input to the decoder is the <SOS> token\n",
    "        decoder_input = target_text[:, 0]\n",
    "        \n",
    "        # Store predictions\n",
    "        outputs = torch.zeros(target_len, batch_size, self.decoder.vocab_size).to(eeg.device)\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target_text[:, t] if teacher_force else top1\n",
    "            \n",
    "        # --- FIX ---\n",
    "        # The outputs tensor has shape [T, B, V] but outputs[0] is all zeros.\n",
    "        # We must slice off this first timestep before permuting to align with the target.\n",
    "        # This changes the shape from [T, B, V] to [T-1, B, V]\n",
    "        return outputs[1:].permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e37df-6ba9-4054-b656-178212500b66",
   "metadata": {},
   "source": [
    "# Phase 4.1: Model Verification (Smoke Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a369b08f-0d92-471a-a9c3-5e651ba59912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated successfully and moved to 'cuda'.\n",
      "Total parameters: 19,001,146\n",
      "\n",
      "Performing one forward pass (smoke test)...\n",
      "Smoke test PASSED.\n",
      "Output logits shape: torch.Size([32, 63, 30522])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "# 1. CRITICAL: Define your vocabulary size. \n",
    "#    Replace this placeholder with the actual number from your tokenizer.\n",
    "VOCAB_SIZE = 30522  # <--- REPLACE THIS VALUE\n",
    "\n",
    "# 2. Set up device and instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(vocab_size=VOCAB_SIZE, pad_id=PAD_ID).to(device)\n",
    "\n",
    "print(f\"Model instantiated successfully and moved to '{device}'.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# 3. Fetch one batch from the train_loader (already created in Phase 3)\n",
    "eeg_batch, text_padded_batch, _ = next(iter(train_loader))\n",
    "\n",
    "# 4. Move batch to the same device as the model\n",
    "eeg_batch = eeg_batch.to(device)\n",
    "text_padded_batch = text_padded_batch.to(device)\n",
    "\n",
    "# 5. Perform a forward pass\n",
    "print(\"\\nPerforming one forward pass (smoke test)...\")\n",
    "with torch.no_grad(): # We don't need to compute gradients for this test\n",
    "    # The model expects the target text for teacher forcing, even if the ratio is low\n",
    "    output_logits = model(eeg_batch, text_padded_batch, teacher_forcing_ratio=0.5)\n",
    "\n",
    "print(\"Smoke test PASSED.\")\n",
    "print(\"Output logits shape:\", output_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf653635-decd-4194-a84e-a06b2c79093d",
   "metadata": {},
   "source": [
    "# Phase 5: The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87a6322c-3e97-4b83-bed7-92fa0ed3f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training (Corrected) ---\n",
      "Epoch 01 | Time: 5m 41s\n",
      "\tTrain Loss: 5.1224\n",
      "\t Val. Loss: 4.5570 | Val. Perplexity: 95.2977\n",
      "\t-> New best model saved!\n",
      "Epoch 02 | Time: 5m 41s\n",
      "\tTrain Loss: 3.9754\n",
      "\t Val. Loss: 4.6639 | Val. Perplexity: 106.0535\n",
      "Epoch 03 | Time: 5m 42s\n",
      "\tTrain Loss: 3.7021\n",
      "\t Val. Loss: 4.5495 | Val. Perplexity: 94.5807\n",
      "\t-> New best model saved!\n",
      "Epoch 04 | Time: 5m 48s\n",
      "\tTrain Loss: 3.5305\n",
      "\t Val. Loss: 4.5962 | Val. Perplexity: 99.1062\n",
      "Epoch 05 | Time: 5m 52s\n",
      "\tTrain Loss: 3.3806\n",
      "\t Val. Loss: 4.5117 | Val. Perplexity: 91.0740\n",
      "\t-> New best model saved!\n",
      "Epoch 06 | Time: 5m 46s\n",
      "\tTrain Loss: 3.2657\n",
      "\t Val. Loss: 4.4999 | Val. Perplexity: 90.0047\n",
      "\t-> New best model saved!\n",
      "Epoch 07 | Time: 5m 45s\n",
      "\tTrain Loss: 3.1515\n",
      "\t Val. Loss: 4.5811 | Val. Perplexity: 97.6246\n",
      "Epoch 08 | Time: 5m 44s\n",
      "\tTrain Loss: 3.0507\n",
      "\t Val. Loss: 4.5562 | Val. Perplexity: 95.2239\n",
      "Epoch 09 | Time: 5m 45s\n",
      "\tTrain Loss: 2.9680\n",
      "\t Val. Loss: 4.6167 | Val. Perplexity: 101.1599\n",
      "Epoch 10 | Time: 5m 44s\n",
      "\tTrain Loss: 2.8499\n",
      "\t Val. Loss: 4.6372 | Val. Perplexity: 103.2523\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- You can run this code now (Corrected Version) ---\n",
    "\n",
    "# We assume you have the following variables defined:\n",
    "# model, train_loader, val_loader, PAD_ID, device\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "def format_time(seconds):\n",
    "    mins = int(seconds // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{mins}m {secs}s\"\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, clip_norm=1.0, teacher_forcing=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for eeg_b, txt_b, _ in loader:\n",
    "        eeg_b = eeg_b.to(device)\n",
    "        txt_b = txt_b.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(eeg_b, txt_b, teacher_forcing_ratio=teacher_forcing)\n",
    "        \n",
    "        output_dim = logits.shape[-1]\n",
    "        # FIX: Changed .view() to .reshape() to handle non-contiguous tensors\n",
    "        logits_flat = logits.reshape(-1, output_dim)\n",
    "        \n",
    "        target_flat = txt_b[:, 1:].reshape(-1)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for eeg_b, txt_b, _ in loader:\n",
    "        eeg_b = eeg_b.to(device)\n",
    "        txt_b = txt_b.to(device)\n",
    "        \n",
    "        logits = model(eeg_b, txt_b, teacher_forcing_ratio=0.0)\n",
    "        \n",
    "        output_dim = logits.shape[-1]\n",
    "        # FIX: Changed .view() to .reshape() to handle non-contiguous tensors\n",
    "        logits_flat = logits.reshape(-1, output_dim)\n",
    "        \n",
    "        target_flat = txt_b[:, 1:].reshape(-1)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"--- Starting Training (Corrected) ---\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    val_perplexity = math.exp(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | Time: {format_time(epoch_time)}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"\\t Val. Loss: {val_loss:.4f} | Val. Perplexity: {val_perplexity:7.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'eeg2text-best-model.pt')\n",
    "        print(\"\\t-> New best model saved!\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b95210-de70-4b3a-acc9-19fa419be860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from local path: /home/poorna/models/bert-base-uncased\n",
      "Best model loaded successfully.\n",
      "\n",
      "--- Running Inference ---\n",
      "\n",
      "GROUND TRUTH: two young elephants playfully wrestle in a grassy field.. tone : playful\n",
      "MODEL PREDICTION: a person of a a a a a a a... tone : calm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer # To convert token IDs back to text\n",
    "\n",
    "# --- You can run this code now (Corrected for local model) ---\n",
    "\n",
    "# 1. Setup: Load Tokenizer and your best model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- FIX ---\n",
    "# Set the path to the directory containing your local BERT model/tokenizer files.\n",
    "# For example: \"C:/Users/YourUser/models/bert-base-uncased\" or \"/home/user/models/bert-base-uncased\"\n",
    "LOCAL_MODEL_PATH = \"/home/poorna/models/bert-base-uncased\" # <--- REPLACE THIS PATH\n",
    "\n",
    "print(f\"Loading tokenizer from local path: {LOCAL_MODEL_PATH}\")\n",
    "# Load the tokenizer from your local folder instead of the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "# --- END OF FIX ---\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "SOS_ID = tokenizer.cls_token_id # BERT's [CLS] is often used as a Start-Of-Sentence token\n",
    "EOS_ID = tokenizer.sep_token_id # BERT's [SEP] is often used as an End-Of-Sentence token\n",
    "\n",
    "# Instantiate a fresh model with the same architecture\n",
    "model = Seq2Seq(vocab_size=VOCAB_SIZE, pad_id=PAD_ID).to(device)\n",
    "\n",
    "# Load the saved weights from your best-performing epoch\n",
    "model.load_state_dict(torch.load('eeg2text-best-model.pt', map_location=device))\n",
    "print(\"Best model loaded successfully.\")\n",
    "\n",
    "\n",
    "# 2. Define the Inference Function (Greedy Decoding)\n",
    "@torch.no_grad()\n",
    "def generate_text(model, eeg_signal):\n",
    "    \"\"\"\n",
    "    Generates a sequence of token IDs from a single EEG signal.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # Add a batch dimension and move to the correct device\n",
    "    eeg_signal = eeg_signal.unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the encoder's output\n",
    "    encoder_outputs, encoder_hidden = model.encoder(eeg_signal)\n",
    "    \n",
    "    # Initialize the decoder hidden state\n",
    "    decoder_hidden = model.decoder.init_hidden(encoder_hidden)\n",
    "    \n",
    "    # Start with the <SOS> token\n",
    "    input_token = torch.tensor([SOS_ID], device=device)\n",
    "    \n",
    "    generated_ids = []\n",
    "    max_len = 100 # Set a max length to prevent infinite loops\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Decode one step\n",
    "        prediction, decoder_hidden, _ = model.decoder(input_token, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # Get the most likely token ID (greedy search)\n",
    "        next_token_id = prediction.argmax(1)\n",
    "        \n",
    "        # If the model predicts the <EOS> token, stop generating\n",
    "        if next_token_id.item() == EOS_ID:\n",
    "            break\n",
    "        \n",
    "        generated_ids.append(next_token_id.item())\n",
    "        \n",
    "        # The predicted token becomes the next input\n",
    "        input_token = next_token_id\n",
    "        \n",
    "    return generated_ids\n",
    "\n",
    "# 3. Run Inference on a sample from the test set\n",
    "# Let's take the 10th sample from your test_ds\n",
    "eeg_sample, true_text_ids = test_ds[10]\n",
    "\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "# Generate the predicted token IDs\n",
    "predicted_ids = generate_text(model, eeg_sample)\n",
    "\n",
    "# Decode the IDs back to human-readable text\n",
    "true_text = tokenizer.decode(true_text_ids, skip_special_tokens=True)\n",
    "predicted_text = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nGROUND TRUTH: {true_text}\")\n",
    "print(f\"MODEL PREDICTION: {predicted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7818d0-a7a7-4277-b1fd-7a9c3019710c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from local path: /home/poorna/models/bert-base-uncased\n",
      "Best model loaded successfully.\n",
      "\n",
      "--- Running Inference with Beam Search ---\n",
      "\n",
      "GROUND TRUTH: two young elephants playfully wrestle in a grassy field.. tone : playful\n",
      "BEAM SEARCH PREDICTION: close - up of a vibrant coral reef.. tone : calm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer # To convert token IDs back to text\n",
    "\n",
    "# --- You can run this code now (Corrected for local model) ---\n",
    "\n",
    "# 1. Setup: Load Tokenizer and your best model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- FIX ---\n",
    "# Set the path to the directory containing your local BERT model/tokenizer files.\n",
    "# For example: \"C:/Users/YourUser/models/bert-base-uncased\" or \"/home/user/models/bert-base-uncased\"\n",
    "LOCAL_MODEL_PATH = \"/home/poorna/models/bert-base-uncased\" # <--- REPLACE THIS PATH\n",
    "\n",
    "print(f\"Loading tokenizer from local path: {LOCAL_MODEL_PATH}\")\n",
    "# Load the tokenizer from your local folder instead of the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "# --- END OF FIX ---\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "SOS_ID = tokenizer.cls_token_id # BERT's [CLS] is often used as a Start-Of-Sentence token\n",
    "EOS_ID = tokenizer.sep_token_id # BERT's [SEP] is often used as an End-Of-Sentence token\n",
    "\n",
    "# Instantiate a fresh model with the same architecture\n",
    "model = Seq2Seq(vocab_size=VOCAB_SIZE, pad_id=PAD_ID).to(device)\n",
    "\n",
    "# Load the saved weights from your best-performing epoch\n",
    "model.load_state_dict(torch.load('eeg2text-best-model.pt', map_location=device))\n",
    "print(\"Best model loaded successfully.\")\n",
    "\n",
    "\n",
    "# (Keep all the model and tokenizer loading code from before)\n",
    "# Just replace the generate_text function with this one\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, eeg_signal, beam_width=5, max_len=100):\n",
    "    model.eval()\n",
    "    eeg_signal = eeg_signal.unsqueeze(0).to(device)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = model.encoder(eeg_signal)\n",
    "    decoder_hidden = model.decoder.init_hidden(encoder_hidden)\n",
    "\n",
    "    # Start with the <SOS> token. A beam is a tuple of (sequence, score, hidden_state)\n",
    "    beams = [([SOS_ID], 0.0, decoder_hidden)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score, hidden in beams:\n",
    "            # If the last token is <EOS>, this beam is finished\n",
    "            if seq[-1] == EOS_ID:\n",
    "                new_beams.append((seq, score, hidden))\n",
    "                continue\n",
    "\n",
    "            # Get the prediction for the next token\n",
    "            input_token = torch.tensor([seq[-1]], device=device)\n",
    "            prediction, new_hidden, _ = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            \n",
    "            # Get the log probabilities for all words in the vocab\n",
    "            log_probs = F.log_softmax(prediction, dim=-1).squeeze()\n",
    "\n",
    "            # Add the top `beam_width` next tokens to our list of candidates\n",
    "            top_log_probs, top_ids = torch.topk(log_probs, beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                new_seq = seq + [top_ids[i].item()]\n",
    "                new_score = score + top_log_probs[i].item()\n",
    "                new_beams.append((new_seq, new_score, new_hidden))\n",
    "\n",
    "        # Sort all candidates by their score (higher is better) and keep the top `beam_width`\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        # If the top beam has ended, we can stop\n",
    "        if beams[0][0][-1] == EOS_ID:\n",
    "            break\n",
    "            \n",
    "    # Return the token IDs from the best beam (first one in the sorted list)\n",
    "    # We strip the initial <SOS> token before returning\n",
    "    return beams[0][0][1:]\n",
    "\n",
    "# --- How to run it ---\n",
    "print(\"\\n--- Running Inference with Beam Search ---\")\n",
    "predicted_ids_beam = beam_search_decode(model, eeg_sample, beam_width=5)\n",
    "predicted_text_beam = tokenizer.decode(predicted_ids_beam, skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nGROUND TRUTH: {true_text}\")\n",
    "print(f\"BEAM SEARCH PREDICTION: {predicted_text_beam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b19907f-646a-42b7-9b55-fd3052b01f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from local path: /home/poorna/models/bert-base-uncased\n",
      "Best model loaded successfully.\n",
      "\n",
      "--- Running Inference on 10 Random Samples ---\n",
      "\n",
      "--- Sample 1/10 (Index: 2708) ---\n",
      "GROUND TRUTH: a woman in a red jacket dances energetically in a room.. tone : energetic\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 2/10 (Index: 1736) ---\n",
      "GROUND TRUTH: a rainbow crepe cake is sliced, revealing colorful layers.. tone : sweet\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 3/10 (Index: 1264) ---\n",
      "GROUND TRUTH: a delicious mushroom and spinach pizza is presented on a metal surface.. tone : appetizing\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 4/10 (Index: 151) ---\n",
      "GROUND TRUTH: a person plays a white kawai digital piano calmly.. tone : calm\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 5/10 (Index: 1217) ---\n",
      "GROUND TRUTH: a man practices boxing in a gym, throwing punches.. tone : focused\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 6/10 (Index: 470) ---\n",
      "GROUND TRUTH: colorful hot air balloons ascend in a sunny sky.. tone : joyful\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 7/10 (Index: 302) ---\n",
      "GROUND TRUTH: a person plays a red drum set in a room.. tone : calm\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 8/10 (Index: 353) ---\n",
      "GROUND TRUTH: two jellyfish drift in a dark aquarium, glowing softly.. tone : serene\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 9/10 (Index: 1572) ---\n",
      "GROUND TRUTH: colorful hot air balloons ascend into a clear blue sky.. tone : serene\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n",
      "\n",
      "--- Sample 10/10 (Index: 163) ---\n",
      "GROUND TRUTH: a tabby cat rests peacefully on a colorful rug.. tone : calm\n",
      "MODEL PREDICTION: close - up of a vibrant coral reef.. tone : calm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import random # Import the random library\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "# 1. Setup: Load Tokenizer and your best model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/home/poorna/models/bert-base-uncased\"\n",
    "\n",
    "print(f\"Loading tokenizer from local path: {LOCAL_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "SOS_ID = tokenizer.cls_token_id\n",
    "EOS_ID = tokenizer.sep_token_id\n",
    "\n",
    "# Instantiate a fresh model with the same architecture\n",
    "# Make sure your Seq2Seq class is defined in a previous cell\n",
    "model = Seq2Seq(vocab_size=VOCAB_SIZE, pad_id=PAD_ID).to(device)\n",
    "\n",
    "# Load the saved weights from your best-performing epoch\n",
    "model.load_state_dict(torch.load('eeg2text-best-model.pt', map_location=device))\n",
    "print(\"Best model loaded successfully.\")\n",
    "\n",
    "\n",
    "# 2. Define the Inference Function (Beam Search)\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, eeg_signal, beam_width=5, max_len=100):\n",
    "    model.eval()\n",
    "    eeg_signal = eeg_signal.unsqueeze(0).to(device)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = model.encoder(eeg_signal)\n",
    "    decoder_hidden = model.decoder.init_hidden(encoder_hidden)\n",
    "\n",
    "    beams = [([SOS_ID], 0.0, decoder_hidden)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score, hidden in beams:\n",
    "            if seq[-1] == EOS_ID:\n",
    "                new_beams.append((seq, score, hidden))\n",
    "                continue\n",
    "\n",
    "            input_token = torch.tensor([seq[-1]], device=device)\n",
    "            prediction, new_hidden, _ = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            \n",
    "            log_probs = F.log_softmax(prediction, dim=-1).squeeze()\n",
    "            top_log_probs, top_ids = torch.topk(log_probs, beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                new_seq = seq + [top_ids[i].item()]\n",
    "                new_score = score + top_log_probs[i].item()\n",
    "                new_beams.append((new_seq, new_score, new_hidden))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        if beams[0][0][-1] == EOS_ID:\n",
    "            break\n",
    "            \n",
    "    return beams[0][0][1:]\n",
    "\n",
    "# --- FIX: Run Inference on 10 Random Samples ---\n",
    "\n",
    "# 3. Select 10 random indices from the test set\n",
    "NUM_SAMPLES = 10\n",
    "test_set_size = len(test_ds)\n",
    "random_indices = random.sample(range(test_set_size), NUM_SAMPLES)\n",
    "\n",
    "print(f\"\\n--- Running Inference on {NUM_SAMPLES} Random Samples ---\")\n",
    "\n",
    "# 4. Loop through the random indices and generate predictions\n",
    "for i, idx in enumerate(random_indices):\n",
    "    eeg_sample, true_text_ids = test_ds[idx]\n",
    "\n",
    "    # Generate the predicted token IDs using beam search\n",
    "    predicted_ids_beam = beam_search_decode(model, eeg_sample, beam_width=5)\n",
    "\n",
    "    # Decode the IDs back to human-readable text\n",
    "    true_text = tokenizer.decode(true_text_ids, skip_special_tokens=True)\n",
    "    predicted_text_beam = tokenizer.decode(predicted_ids_beam, skip_special_tokens=True)\n",
    "    \n",
    "    # Print the results for this sample\n",
    "    print(f\"\\n--- Sample {i+1}/{NUM_SAMPLES} (Index: {idx}) ---\")\n",
    "    print(f\"GROUND TRUTH: {true_text}\")\n",
    "    print(f\"MODEL PREDICTION: {predicted_text_beam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa3085-0e15-4474-9c5b-a7cc2f2c50c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
