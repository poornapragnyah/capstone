{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510ea4fa-867a-444a-bfb1-c2b9483dc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 0: Establishing the Data Foundation ---\n",
      "Loading tokenizer from: /home/poorna/models/bert-base-uncased\n",
      "Tokenizer loaded. PAD_ID set to: 0\n",
      "\n",
      "Creating Dataset and splitting into train/val/test sets...\n",
      "Split sizes -> Train: 22400, Val: 2800, Test: 2800\n",
      "Creating DataLoaders...\n",
      "DataLoaders created successfully.\n",
      "\n",
      "--- Verification: Inspecting one batch from train_loader ---\n",
      "EEG batch shape: torch.Size([32, 62, 400])\n",
      "Metadata batch shape: torch.Size([32, 3])\n",
      "Text batch shape: torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "print(\"--- Phase 0: Establishing the Data Foundation ---\")\n",
    "\n",
    "# 1. Define Constants using the context we've confirmed\n",
    "H5_FILE_PATH = \"/home/poorna/data/dataset.h5\"\n",
    "LOCAL_MODEL_PATH = \"/home/poorna/models/bert-base-uncased\"\n",
    "TRAIN_PCT, VAL_PCT = 0.8, 0.1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 2. Load Tokenizer to get PAD_ID\n",
    "print(f\"Loading tokenizer from: {LOCAL_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "print(f\"Tokenizer loaded. PAD_ID set to: {PAD_ID}\")\n",
    "\n",
    "# 3. Define the custom PyTorch Dataset\n",
    "class EEGMetaTextH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads EEG, Metadata, and Text (input_ids) from the specified HDF5 file.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path):\n",
    "        self.h5_path = h5_path\n",
    "        self.h5_file = None\n",
    "        # Get the number of samples from the file once\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            self.n_samples = f['eeg'].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open file handle in __getitem__ for better compatibility with multiprocessing\n",
    "        if self.h5_file is None:\n",
    "            self.h5_file = h5py.File(self.h5_path, 'r')\n",
    "        \n",
    "        # Load data using the confirmed keys\n",
    "        eeg = torch.from_numpy(self.h5_file['eeg'][idx].astype('float32'))\n",
    "        meta = torch.from_numpy(self.h5_file['metadata'][idx].astype('float32'))\n",
    "        text = torch.from_numpy(self.h5_file['input_ids'][idx].astype('int64'))\n",
    "        \n",
    "        return eeg, meta, text\n",
    "\n",
    "# 4. Define the custom Collate Function for batching\n",
    "def collate_multimodal_batch(batch):\n",
    "    \"\"\"\n",
    "    Collates samples into a batch, stacking fixed-size tensors and padding variable-length ones.\n",
    "    \"\"\"\n",
    "    eeg_list, meta_list, text_list = [], [], []\n",
    "    for eeg, meta, txt in batch:\n",
    "        eeg_list.append(eeg)\n",
    "        meta_list.append(meta)\n",
    "        text_list.append(txt)\n",
    "\n",
    "    # EEG and Metadata are fixed-size, so we can stack them\n",
    "    eeg_batch = torch.stack(eeg_list, dim=0)\n",
    "    meta_batch = torch.stack(meta_list, dim=0)\n",
    "    \n",
    "    # Text (input_ids) is variable-length and needs padding\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=PAD_ID)\n",
    "\n",
    "    return eeg_batch.float(), meta_batch.float(), text_padded\n",
    "\n",
    "# 5. Create Dataset instance, perform splits, and create DataLoaders\n",
    "print(\"\\nCreating Dataset and splitting into train/val/test sets...\")\n",
    "dataset = EEGMetaTextH5Dataset(H5_FILE_PATH)\n",
    "\n",
    "N = len(dataset)\n",
    "n_train = int(N * TRAIN_PCT)\n",
    "n_val   = int(N * VAL_PCT)\n",
    "n_test  = N - n_train - n_val\n",
    "g = torch.Generator().manual_seed(42)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=g)\n",
    "print(f\"Split sizes -> Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_multimodal_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_multimodal_batch)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_multimodal_batch)\n",
    "print(\"DataLoaders created successfully.\")\n",
    "\n",
    "# 6. Final Verification Step\n",
    "print(\"\\n--- Verification: Inspecting one batch from train_loader ---\")\n",
    "eeg_b, meta_b, text_b = next(iter(train_loader))\n",
    "print(\"EEG batch shape:\", eeg_b.shape)\n",
    "print(\"Metadata batch shape:\", meta_b.shape)\n",
    "print(\"Text batch shape:\", text_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fa48502-e6ca-42ed-ab8b-13b4b15e1be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Defining the Multimodal Model Architecture ---\n",
      "\n",
      "--- Phase 1: Model Architecture Defined Successfully ---\n",
      "All classes (LuongAttention, MetadataEncoder, EEGEncoder, Decoder, Seq2Seq) are now defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "print(\"--- Phase 1: Defining the Multimodal Model Architecture ---\")\n",
    "\n",
    "# We define all necessary components here for a fresh start.\n",
    "\n",
    "# Component 1: The Attention Mechanism\n",
    "class LuongAttention(nn.Module):\n",
    "    \"\"\" Implements Luong's general dot-product attention. \"\"\"\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim, dec_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden shape: [1, B, dec_dim]\n",
    "        # encoder_outputs shape: [T_enc, B, enc_dim]\n",
    "        scores = torch.bmm(self.attn(encoder_outputs).permute(1, 0, 2), decoder_hidden.permute(1, 2, 0))\n",
    "        attn_weights = F.softmax(scores, dim=1)\n",
    "        context = torch.bmm(attn_weights.permute(0, 2, 1), encoder_outputs.permute(1, 0, 2))\n",
    "        return context, attn_weights.squeeze(-1)\n",
    "\n",
    "# Component 2: The new MetadataEncoder\n",
    "class MetadataEncoder(nn.Module):\n",
    "    \"\"\"Encodes structured metadata [color, category, motion] into a single feature vector.\"\"\"\n",
    "    def __init__(self, num_colors, num_categories, color_emb_dim=16, category_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.color_embedding = nn.Embedding(num_colors, color_emb_dim)\n",
    "        self.category_embedding = nn.Embedding(num_categories, category_emb_dim)\n",
    "        # The final output dimension is the sum of the two embedding dims + 1 for the motion float\n",
    "        self.output_dim = color_emb_dim + category_emb_dim + 1\n",
    "\n",
    "    def forward(self, metadata):\n",
    "        # metadata shape: [Batch, 3] -> [color_id, category_id, motion_value]\n",
    "        color_ids = metadata[:, 0].long()      # Convert float IDs to long for embedding\n",
    "        category_ids = metadata[:, 1].long()   # Convert float IDs to long for embedding\n",
    "        motion_values = metadata[:, 2].unsqueeze(1) # Keep as float, add feature dim\n",
    "\n",
    "        color_vec = self.color_embedding(color_ids)\n",
    "        category_vec = self.category_embedding(category_ids)\n",
    "\n",
    "        # Combine all features into a single vector\n",
    "        combined_features = torch.cat([color_vec, category_vec, motion_values], dim=1)\n",
    "        return combined_features\n",
    "\n",
    "# Component 3: The EEGEncoder (unchanged)\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=62, conv_dim=128, enc_hidden=256, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 128, 5, 1, 2), nn.BatchNorm1d(128), nn.ReLU(True), nn.Dropout(dropout),\n",
    "            nn.Conv1d(128, conv_dim, 5, 1, 2), nn.BatchNorm1d(conv_dim), nn.ReLU(True), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.rnn = nn.GRU(conv_dim, enc_hidden, num_layers, bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "    def forward(self, eeg):\n",
    "        x = self.conv(eeg).permute(2, 0, 1)\n",
    "        return self.rnn(x)\n",
    "\n",
    "# Component 4: The updated Decoder\n",
    "# In your Phase 1 (Model Architecture) cell, replace the old Decoder class with this one.\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, enc_hidden, dec_hidden, meta_features_dim, num_layers, pad_id, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- FIX: Added this line ---\n",
    "        self.vocab_size = vocab_size\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        enc_dim = enc_hidden * 2\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.attention = LuongAttention(enc_dim, dec_hidden)\n",
    "        self.rnn = nn.GRU(emb_dim + enc_dim, dec_hidden, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc_out = nn.Linear(dec_hidden, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bridge = nn.Linear(enc_dim + meta_features_dim, dec_hidden)\n",
    "\n",
    "    def init_hidden(self, combined_features):\n",
    "        return torch.tanh(self.bridge(combined_features))\n",
    "\n",
    "    def forward(self, token, decoder_hidden, encoder_outputs):\n",
    "        token = token.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(token))\n",
    "        context, _ = self.attention(decoder_hidden[-1].unsqueeze(0), encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context.permute(1,0,2)), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, decoder_hidden)\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden\n",
    "\n",
    "# Component 5: The main Seq2Seq class tying it all together\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, text_vocab_size, num_colors, num_categories, enc_hidden=256, dec_hidden=256, \n",
    "                 pad_id=0, dropout=0.2, color_emb_dim=16, category_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = EEGEncoder(enc_hidden=enc_hidden, dropout=dropout)\n",
    "        self.meta_encoder = MetadataEncoder(num_colors, num_categories, color_emb_dim, category_emb_dim)\n",
    "        \n",
    "        meta_features_dim = self.meta_encoder.output_dim\n",
    "        \n",
    "        self.decoder = Decoder(text_vocab_size, 256, enc_hidden, dec_hidden, \n",
    "                               meta_features_dim, 2, pad_id, dropout)\n",
    "\n",
    "    def forward(self, eeg, metadata, target_text, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(eeg)\n",
    "        meta_features = self.meta_encoder(metadata)\n",
    "        \n",
    "        num_layers = self.decoder.rnn.num_layers\n",
    "        forward_h = encoder_hidden[0::2]\n",
    "        backward_h = encoder_hidden[1::2]\n",
    "        encoder_hidden_cat = torch.cat([forward_h, backward_h], dim=2)\n",
    "        \n",
    "        meta_features_repeated = meta_features.unsqueeze(0).repeat(num_layers, 1, 1)\n",
    "        combined_features = torch.cat([encoder_hidden_cat, meta_features_repeated], dim=2)\n",
    "        \n",
    "        decoder_hidden = self.decoder.init_hidden(combined_features)\n",
    "        \n",
    "        batch_size = eeg.shape[0]\n",
    "        target_len = target_text.shape[1]\n",
    "        outputs = torch.zeros(target_len, batch_size, self.decoder.vocab_size).to(eeg.device)\n",
    "        decoder_input = target_text[:, 0]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target_text[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs[1:].permute(1, 0, 2)\n",
    "\n",
    "print(\"\\n--- Phase 1: Model Architecture Defined Successfully ---\")\n",
    "print(\"All classes (LuongAttention, MetadataEncoder, EEGEncoder, Decoder, Seq2Seq) are now defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ce4513-3cbc-40c3-a4fe-eaa62f62a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 2: Verifying the Complete Architecture ---\n",
      "New dual-input model instantiated on 'cuda'.\n",
      "Total parameters: 19,016,618\n",
      "\n",
      "--- Performing Smoke Test Forward Pass ---\n",
      "\n",
      "Smoke test PASSED.\n",
      "Output logits shape: torch.Size([32, 63, 30522])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- You can run this corrected code now ---\n",
    "\n",
    "print(\"--- Phase 2: Verifying the Complete Architecture ---\")\n",
    "\n",
    "# 1. Define Model Parameters\n",
    "TEXT_VOCAB_SIZE = 30522\n",
    "NUM_COLORS = 77\n",
    "NUM_CATEGORIES = 53\n",
    "\n",
    "# 2. Instantiate the Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Seq2Seq(\n",
    "    text_vocab_size=TEXT_VOCAB_SIZE,\n",
    "    num_colors=NUM_COLORS,\n",
    "    num_categories=NUM_CATEGORIES,\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "print(f\"New dual-input model instantiated on '{device}'.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# 3. Perform the Forward Pass\n",
    "eeg_b, meta_b, text_b = next(iter(train_loader))\n",
    "eeg_b, meta_b, text_b = eeg_b.to(device), meta_b.to(device), text_b.to(device)\n",
    "\n",
    "print(\"\\n--- Performing Smoke Test Forward Pass ---\")\n",
    "with torch.no_grad():\n",
    "    logits = model(eeg_b, meta_b, text_b)\n",
    "\n",
    "print(\"\\nSmoke test PASSED.\")\n",
    "print(\"Output logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3a382-7cb6-4597-8bcd-358818f0422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A Quick Guide to Our Tensor Dimensions ---\n",
    "\n",
    "# Let's define the meaning of each letter we see in the shapes:\n",
    "# B = Batch Size (e.g., 32)\n",
    "# C = EEG Channels (e.g., 62)\n",
    "# T_eeg = EEG Timesteps (e.g., 400)\n",
    "# M = Number of Metadata Features (e.g., 3 for color, category, motion)\n",
    "# T_text = Max Text Sequence Length in the Batch (e.g., 64)\n",
    "# V = Text Vocabulary Size (e.g., 30522)\n",
    "\n",
    "# --- Tensors coming FROM the DataLoader ---\n",
    "# eeg_b.shape:      [B, C, T_eeg] -> e.g., [32, 62, 400]\n",
    "# meta_b.shape:     [B, M]        -> e.g., [32, 3]\n",
    "# text_b.shape:     [B, T_text]   -> e.g., [32, 64]\n",
    "\n",
    "# --- Tensor coming FROM the Model ---\n",
    "# logits.shape:     [B, T_text - 1, V] -> e.g., [32, 63, 30522]\n",
    "#\n",
    "# Why T_text - 1?\n",
    "# The model's task is to predict the *next* token. For an input text\n",
    "# sequence of length N, there are N-1 \"next tokens\" to predict.\n",
    "# Example: For input [<sos>, \"the\", \"cat\", <eos>], the model predicts\n",
    "# [\"the\", \"cat\", <eos>], which is 3 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f49e40-06bc-498f-bdf0-9980e7a2dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 3: Launching the Training Protocol ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poorna/venvs/torch/lib64/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "\n",
      "Epoch 01/20 | Time: 6m 0s\n",
      "\tTrain Loss: 5.1146\n",
      "\t Val. Loss: 4.5608 | Val. Perplexity: 95.6605\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 02/20 | Time: 5m 53s\n",
      "\tTrain Loss: 3.9787\n",
      "\t Val. Loss: 4.6229 | Val. Perplexity: 101.7841\n",
      "\n",
      "Epoch 03/20 | Time: 5m 55s\n",
      "\tTrain Loss: 3.7195\n",
      "\t Val. Loss: 4.5368 | Val. Perplexity: 93.3917\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 04/20 | Time: 5m 55s\n",
      "\tTrain Loss: 3.5073\n",
      "\t Val. Loss: 4.5856 | Val. Perplexity: 98.0653\n",
      "\n",
      "Epoch 05/20 | Time: 5m 54s\n",
      "\tTrain Loss: 3.3098\n",
      "\t Val. Loss: 4.4361 | Val. Perplexity: 84.4438\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 06/20 | Time: 5m 53s\n",
      "\tTrain Loss: 3.1079\n",
      "\t Val. Loss: 4.6393 | Val. Perplexity: 103.4735\n",
      "\n",
      "Epoch 07/20 | Time: 5m 55s\n",
      "\tTrain Loss: 2.9481\n",
      "\t Val. Loss: 4.4717 | Val. Perplexity: 87.5074\n",
      "\n",
      "Epoch 08/20 | Time: 5m 58s\n",
      "\tTrain Loss: 2.7686\n",
      "\t Val. Loss: 4.4347 | Val. Perplexity: 84.3228\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 09/20 | Time: 5m 53s\n",
      "\tTrain Loss: 2.6266\n",
      "\t Val. Loss: 4.3328 | Val. Perplexity: 76.1548\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 10/20 | Time: 5m 51s\n",
      "\tTrain Loss: 2.4851\n",
      "\t Val. Loss: 4.2894 | Val. Perplexity: 72.9199\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 11/20 | Time: 5m 50s\n",
      "\tTrain Loss: 2.3773\n",
      "\t Val. Loss: 4.3517 | Val. Perplexity: 77.6129\n",
      "\n",
      "Epoch 12/20 | Time: 5m 50s\n",
      "\tTrain Loss: 2.2794\n",
      "\t Val. Loss: 4.3598 | Val. Perplexity: 78.2422\n",
      "\n",
      "Epoch 13/20 | Time: 5m 50s\n",
      "\tTrain Loss: 2.1804\n",
      "\t Val. Loss: 4.2653 | Val. Perplexity: 71.1889\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 14/20 | Time: 5m 50s\n",
      "\tTrain Loss: 2.0887\n",
      "\t Val. Loss: 4.1994 | Val. Perplexity: 66.6453\n",
      "\t-> Validation loss improved, saving new best model.\n",
      "\n",
      "Epoch 15/20 | Time: 6m 18s\n",
      "\tTrain Loss: 2.0286\n",
      "\t Val. Loss: 4.3309 | Val. Perplexity: 76.0164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m     68\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     val_loss = evaluate(model, val_loader, criterion)\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Update the learning rate based on validation loss\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Reshape for loss calculation\u001b[39;00m\n\u001b[32m     38\u001b[39m loss = criterion(logits.reshape(-\u001b[32m1\u001b[39m, logits.shape[-\u001b[32m1\u001b[39m]), txt_b[:, \u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     42\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch/lib64/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch/lib64/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch/lib64/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "import math\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "print(\"--- Phase 3: Launching the Training Protocol ---\")\n",
    "\n",
    "# We assume 'model', 'train_loader', 'val_loader', 'PAD_ID', and 'device' are defined and in memory.\n",
    "\n",
    "# 1. Define Loss Function, Optimizer, and Scheduler\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "# Scheduler will reduce the learning rate if validation loss stops improving\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=2, verbose=True)\n",
    "\n",
    "# 2. Define a utility function for timing\n",
    "def format_time(seconds):\n",
    "    mins = int(seconds // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{mins}m {secs}s\"\n",
    "\n",
    "# 3. Define the Training Function for one epoch (updated for metadata)\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for eeg_b, meta_b, txt_b in loader:\n",
    "        eeg_b, meta_b, txt_b = eeg_b.to(device), meta_b.to(device), txt_b.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Pass all three inputs to the model\n",
    "        logits = model(eeg_b, meta_b, txt_b, teacher_forcing_ratio=0.5)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), txt_b[:, 1:].reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 4. Define the Evaluation Function (updated for metadata)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for eeg_b, meta_b, txt_b in loader:\n",
    "        eeg_b, meta_b, txt_b = eeg_b.to(device), meta_b.to(device), txt_b.to(device)\n",
    "        # Pass all three inputs to the model\n",
    "        logits = model(eeg_b, meta_b, txt_b, teacher_forcing_ratio=0.0)\n",
    "        \n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), txt_b[:, 1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 5. The Main Training Loop\n",
    "EPOCHS = 20  # You can adjust this number\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch:02d}/{EPOCHS} | Time: {format_time(end_time - start_time)}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"\\t Val. Loss: {val_loss:.4f} | Val. Perplexity: {math.exp(val_loss):7.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'eeg-meta-text-best-model.pt')\n",
    "        print(\"\\t-> Validation loss improved, saving new best model.\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd507be-3713-445c-8dc4-d3d01fca1f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 4: Inference with Dual-Input Model ---\n",
      "Loading tokenizer from local path: /home/poorna/models/bert-base-uncased\n",
      "Loading saved weights from: eeg-meta-text-best-model.pt\n",
      "Best dual-input model loaded successfully.\n",
      "\n",
      "--- Running Inference on 10 Random Samples ---\n",
      "\n",
      "--- Sample 1/10 (Index: 539) ---\n",
      "GROUND TRUTH: water sprays onto a bunch of ripe bananas.. tone : calm\n",
      "MODEL PREDICTION: close - up of a bunch of ripe yellow bananas.. tone : calm\n",
      "\n",
      "--- Sample 2/10 (Index: 2286) ---\n",
      "GROUND TRUTH: colorful fish swim around vibrant coral in a clear ocean.. tone : serene\n",
      "MODEL PREDICTION: colorful hot air balloons float in a clear blue sky.. tone : serene\n",
      "\n",
      "--- Sample 3/10 (Index: 597) ---\n",
      "GROUND TRUTH: motorcyclists race around a track during a competition.. tone : energetic\n",
      "MODEL PREDICTION: a motorcyclist rides a snowy mountain road.. tone : energetic\n",
      "\n",
      "--- Sample 4/10 (Index: 870) ---\n",
      "GROUND TRUTH: three panda cubs playfully tumble on a grassy lawn.. tone : playful\n",
      "MODEL PREDICTION: a giant panda eats bamboo in a lush green forest.. tone : calm\n",
      "\n",
      "--- Sample 5/10 (Index: 1471) ---\n",
      "GROUND TRUTH: gentle waves lap a dark sand beach near rocky outcrops.. tone : serene\n",
      "MODEL PREDICTION: sunlight streams through a dense pine forest.. tone : serene\n",
      "\n",
      "--- Sample 6/10 (Index: 1424) ---\n",
      "GROUND TRUTH: a cluster of small, light - brown mushrooms grows on a mossy log in a forest.. tone : serene\n",
      "MODEL PREDICTION: close - up view of vibrant pink azaleas blossoms.. tone : serene\n",
      "\n",
      "--- Sample 7/10 (Index: 2459) ---\n",
      "GROUND TRUTH: a herd of horses gallops energetically along a sandy beach.. tone : energetic\n",
      "MODEL PREDICTION: a herd of horses gallops along a sandy beach at sunset.. tone : energetic\n",
      "\n",
      "--- Sample 8/10 (Index: 800) ---\n",
      "GROUND TRUTH: close - up of delicate white cherry blossoms on a branch.. tone : serene\n",
      "MODEL PREDICTION: water cascades over dark rocks in a mountain stream.. tone : calm\n",
      "\n",
      "--- Sample 9/10 (Index: 2577) ---\n",
      "GROUND TRUTH: gentle waves roll onto a sandy beach at sunset.. tone : serene\n",
      "MODEL PREDICTION: vibrant coral reef teeming with colorful fish.. tone : serene\n",
      "\n",
      "--- Sample 10/10 (Index: 2593) ---\n",
      "GROUND TRUTH: herd of icelandic horses graze in a lush green field.. tone : serene\n",
      "MODEL PREDICTION: a herd of horses gallops along a sandy beach at sunset.. tone : energetic\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "\n",
    "# --- You can run this code now ---\n",
    "\n",
    "print(\"--- Phase 4: Inference with Dual-Input Model ---\")\n",
    "\n",
    "# 1. Setup: Load Tokenizer and your new best model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/home/poorna/models/bert-base-uncased\"\n",
    "print(f\"Loading tokenizer from local path: {LOCAL_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Define all necessary constants for model instantiation\n",
    "TEXT_VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "SOS_ID = tokenizer.cls_token_id\n",
    "EOS_ID = tokenizer.sep_token_id\n",
    "NUM_COLORS = 77\n",
    "NUM_CATEGORIES = 53\n",
    "\n",
    "# --- MODIFICATION: Instantiate the correct dual-input Seq2Seq model ---\n",
    "# We assume the Seq2Seq class and its components are defined from your (Revised) Phase 1.\n",
    "model = Seq2Seq(\n",
    "    text_vocab_size=TEXT_VOCAB_SIZE,\n",
    "    num_colors=NUM_COLORS,\n",
    "    num_categories=NUM_CATEGORIES,\n",
    "    pad_id=PAD_ID,\n",
    "    dropout=0.4 # Use the same dropout as the trained model\n",
    ").to(device)\n",
    "\n",
    "# --- MODIFICATION: Load the correct checkpoint file ---\n",
    "checkpoint_path = 'eeg-meta-text-best-model.pt'\n",
    "print(f\"Loading saved weights from: {checkpoint_path}\")\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "print(\"Best dual-input model loaded successfully.\")\n",
    "\n",
    "\n",
    "# 2. Define the Updated Inference Function\n",
    "@torch.no_grad()\n",
    "def generate_with_metadata(model, eeg_signal, meta_signal, beam_width=5, max_len=100):\n",
    "    \"\"\"\n",
    "    Generates text using beam search, accepting both EEG and Metadata as input.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    eeg_signal = eeg_signal.unsqueeze(0).to(device)\n",
    "    meta_signal = meta_signal.unsqueeze(0).to(device)\n",
    "\n",
    "    # --- MODIFICATION: Use both encoders and fuse the features ---\n",
    "    encoder_outputs, encoder_hidden = model.encoder(eeg_signal)\n",
    "    meta_features = model.meta_encoder(meta_signal)\n",
    "    \n",
    "    num_layers = model.decoder.rnn.num_layers\n",
    "    forward_h = encoder_hidden[0::2]\n",
    "    backward_h = encoder_hidden[1::2]\n",
    "    encoder_hidden_cat = torch.cat([forward_h, backward_h], dim=2)\n",
    "    \n",
    "    meta_features_repeated = meta_features.unsqueeze(0).repeat(num_layers, 1, 1)\n",
    "    combined_features = torch.cat([encoder_hidden_cat, meta_features_repeated], dim=2)\n",
    "    \n",
    "    # Initialize the decoder's hidden state with the fused features\n",
    "    decoder_hidden = model.decoder.init_hidden(combined_features)\n",
    "    # --- END OF MODIFICATION ---\n",
    "\n",
    "    # The beam search decoding loop remains the same\n",
    "    beams = [([SOS_ID], 0.0, decoder_hidden)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score, hidden in beams:\n",
    "            if seq[-1] == EOS_ID:\n",
    "                new_beams.append((seq, score, hidden))\n",
    "                continue\n",
    "\n",
    "            input_token = torch.tensor([seq[-1]], device=device)\n",
    "            # The decoder's forward pass only needs the token, hidden state, and EEG outputs for attention\n",
    "            prediction, new_hidden = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            \n",
    "            log_probs = F.log_softmax(prediction, dim=-1).squeeze()\n",
    "            top_log_probs, top_ids = torch.topk(log_probs, beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                new_seq = seq + [top_ids[i].item()]\n",
    "                new_score = score + top_log_probs[i].item()\n",
    "                new_beams.append((new_seq, new_score, new_hidden))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        if beams[0][0][-1] == EOS_ID:\n",
    "            break\n",
    "            \n",
    "    return beams[0][0][1:]\n",
    "\n",
    "# 3. Run Inference on 10 Random Samples\n",
    "NUM_SAMPLES = 10\n",
    "# We assume 'test_ds' is in memory from your Phase 0 data setup\n",
    "test_set_size = len(test_ds)\n",
    "random_indices = random.sample(range(test_set_size), NUM_SAMPLES)\n",
    "\n",
    "print(f\"\\n--- Running Inference on {NUM_SAMPLES} Random Samples ---\")\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # --- MODIFICATION: Fetch all three data components ---\n",
    "    eeg_sample, meta_sample, true_text_ids = test_ds[idx]\n",
    "\n",
    "    # --- MODIFICATION: Pass both EEG and Metadata to the generation function ---\n",
    "    predicted_ids_beam = generate_with_metadata(model, eeg_sample, meta_sample, beam_width=5)\n",
    "\n",
    "    # Decode the IDs back to human-readable text\n",
    "    true_text = tokenizer.decode(true_text_ids, skip_special_tokens=True)\n",
    "    predicted_text_beam = tokenizer.decode(predicted_ids_beam, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n--- Sample {i+1}/{NUM_SAMPLES} (Index: {idx}) ---\")\n",
    "    print(f\"GROUND TRUTH: {true_text}\")\n",
    "    print(f\"MODEL PREDICTION: {predicted_text_beam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a97993-7fc0-4880-98cf-5f1c1ed4bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
