{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1eb76f-7d23-4df7-a274-47dd29a05d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 EEG files → Total samples: 28000\n",
      "Loaded 1400 metadata JSON files\n",
      "Scene categories: 77 | Colors: 53\n",
      "\n",
      "EEG batch: torch.Size([32, 62, 400])\n",
      "Input IDs: torch.Size([32, 64])\n",
      "Metadata batch: torch.Size([32, 3])\n",
      "\n",
      "Sample Inspection:\n",
      "EEG shape: torch.Size([62, 400])\n",
      "Token IDs: tensor([  101, 14231,  2980,  2250, 22163, 14257,  2114,  1037,  3154,  2630,\n",
      "         3712,  1012,  1012,  4309,  1024, 25388,   102,     0,     0,     0])\n",
      "Decoded caption: colorful hot air balloons float against a clear blue sky.. tone : serene\n",
      "Metadata tensor: tensor([1.0000, 3.0000, 0.1000])\n",
      "Scene: aerial, Color: blue, Motion: 0.100\n",
      "\n",
      "Total samples loaded: 28000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class EEGTextMetaDataset(Dataset):\n",
    "    def __init__(self, eeg_dir, metadata_dir, tokenizer, max_length=64, use_emotional_tone=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_emotional_tone = use_emotional_tone\n",
    "\n",
    "        # -------------------------\n",
    "        # 1. Load EEG files (all subjects)\n",
    "        # -------------------------\n",
    "        eeg_files = []\n",
    "        for root, dirs, files in os.walk(eeg_dir):\n",
    "            for f in files:\n",
    "                if f.endswith(\".npy\") and \"_preprocessed\" in f:\n",
    "                    eeg_files.append(os.path.join(root, f))\n",
    "        eeg_files = sorted(eeg_files)\n",
    "\n",
    "        if not eeg_files:\n",
    "            raise FileNotFoundError(f\"No EEG .npy files found in {eeg_dir}\")\n",
    "\n",
    "        self.eeg_file_paths = eeg_files\n",
    "        self.eeg_data_list = []\n",
    "        self.index_map = []\n",
    "\n",
    "        for subj_idx, path in enumerate(self.eeg_file_paths):\n",
    "            eeg = np.load(path, mmap_mode='r')\n",
    "\n",
    "            # --- Assertion 1: EEG shape ---\n",
    "            assert eeg.ndim == 3 and eeg.shape[1:] == (62, 400), \\\n",
    "                f\"EEG file {path} has shape {eeg.shape}, expected (*, 62, 400)\"\n",
    "\n",
    "            self.eeg_data_list.append(eeg)\n",
    "\n",
    "            # build index map\n",
    "            n_samples = eeg.shape[0]\n",
    "            self.index_map.extend([(subj_idx, i) for i in range(n_samples)])\n",
    "\n",
    "        total_samples = len(self.index_map)\n",
    "        print(f\"Found {len(self.eeg_file_paths)} EEG files → Total samples: {total_samples}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Load Metadata JSONs\n",
    "        # -------------------------\n",
    "        metadata_files = sorted(\n",
    "            [os.path.join(dp, f)\n",
    "             for dp, dn, filenames in os.walk(metadata_dir)\n",
    "             for f in filenames if f.endswith(\".json\")]\n",
    "        )\n",
    "        if not metadata_files:\n",
    "            raise FileNotFoundError(f\"No metadata JSON files found in {metadata_dir}\")\n",
    "\n",
    "        self.metadata_list = []\n",
    "        for fpath in metadata_files:\n",
    "            with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                meta = json.load(f)\n",
    "\n",
    "                # --- Assertion 2: Metadata content validity ---\n",
    "                assert \"semantic_features\" in meta and \"scene_category\" in meta[\"semantic_features\"], \\\n",
    "                    f\"Missing scene_category in {fpath}\"\n",
    "                assert \"visual_attributes\" in meta and \"major_colors\" in meta[\"visual_attributes\"], \\\n",
    "                    f\"Missing major_colors in {fpath}\"\n",
    "                assert \"optical_flow_score\" in meta[\"visual_attributes\"], \\\n",
    "                    f\"Missing optical_flow_score in {fpath}\"\n",
    "\n",
    "                self.metadata_list.append(meta)\n",
    "\n",
    "        base_count = len(self.metadata_list)\n",
    "        print(f\"Loaded {base_count} metadata JSON files\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Build base captions\n",
    "        # -------------------------\n",
    "        base_captions = []\n",
    "        for meta in self.metadata_list:\n",
    "            caption_text = meta[\"caption\"][\"text\"]\n",
    "            if self.use_emotional_tone and \"emotional_tone\" in meta[\"caption\"]:\n",
    "                caption_text += f\". Tone: {meta['caption']['emotional_tone']}\"\n",
    "            base_captions.append(caption_text)\n",
    "\n",
    "        # --- Assertion 3: captions vs metadata ---\n",
    "        assert len(base_captions) == len(self.metadata_list), \\\n",
    "            \"Mismatch between metadata and base captions count\"\n",
    "\n",
    "        # Repeat for each subject\n",
    "        num_subjects = len(self.eeg_file_paths)\n",
    "        self.captions = base_captions * num_subjects\n",
    "        self.metadata_repeated = self.metadata_list * num_subjects\n",
    "\n",
    "        # --- Assertion 4: Repeated counts ---\n",
    "        assert len(self.captions) == len(self.metadata_repeated) == len(self.index_map), \\\n",
    "            \"Mismatch after repeating captions and metadata for subjects\"\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Encode metadata categories\n",
    "        # -------------------------\n",
    "        scene_categories = sorted(list({m[\"semantic_features\"][\"scene_category\"] for m in self.metadata_list}))\n",
    "        colors = sorted(list({m[\"visual_attributes\"][\"major_colors\"][0][\"color\"].split()[0]\n",
    "                              for m in self.metadata_list}))\n",
    "\n",
    "        self.scene_to_id = {scene: i for i, scene in enumerate(scene_categories)}\n",
    "        self.color_to_id = {c: i for i, c in enumerate(colors)}\n",
    "\n",
    "        print(f\"Scene categories: {len(self.scene_to_id)} | Colors: {len(self.color_to_id)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subj_idx, local_idx = self.index_map[idx]\n",
    "\n",
    "        # 1. EEG tensor\n",
    "        eeg_tensor = torch.tensor(self.eeg_data_list[subj_idx][local_idx], dtype=torch.float32)  # (62, 400)\n",
    "\n",
    "        # 2. Tokenized caption\n",
    "        caption = self.captions[idx]\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "        # 3. Metadata tensor\n",
    "        meta = self.metadata_repeated[idx]\n",
    "        scene_id = self.scene_to_id[meta[\"semantic_features\"][\"scene_category\"]]\n",
    "        color_id = self.color_to_id[meta[\"visual_attributes\"][\"major_colors\"][0][\"color\"].split()[0]]\n",
    "        motion_score = float(meta[\"visual_attributes\"][\"optical_flow_score\"][\"value\"])\n",
    "        metadata_tensor = torch.tensor([scene_id, color_id, motion_score], dtype=torch.float32)\n",
    "\n",
    "        return eeg_tensor, tokenized, metadata_tensor\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Test the dataset\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"/home/poorna/models/bert-base-uncased\")\n",
    "\n",
    "    EEG_DIR = \"/home/poorna/data/preprocessed_eeg\"\n",
    "    METADATA_DIR = \"/home/poorna/data/metadata_dir\"\n",
    "\n",
    "    dataset = EEGTextMetaDataset(\n",
    "        eeg_dir=EEG_DIR,\n",
    "        metadata_dir=METADATA_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    for eeg_batch, tokenized_batch, meta_batch in loader:\n",
    "        print(\"\\nEEG batch:\", eeg_batch.shape)\n",
    "        print(\"Input IDs:\", tokenized_batch[\"input_ids\"].shape)\n",
    "        print(\"Metadata batch:\", meta_batch.shape)\n",
    "\n",
    "        # Sample inspection\n",
    "        sample_idx = 0\n",
    "        print(\"\\nSample Inspection:\")\n",
    "        print(\"EEG shape:\", eeg_batch[sample_idx].shape)\n",
    "        print(\"Token IDs:\", tokenized_batch[\"input_ids\"][sample_idx][:20])\n",
    "        print(\"Decoded caption:\", tokenizer.decode(\n",
    "            tokenized_batch[\"input_ids\"][sample_idx], skip_special_tokens=True\n",
    "        ))\n",
    "        print(\"Metadata tensor:\", meta_batch[sample_idx])\n",
    "\n",
    "        scene_id = int(meta_batch[sample_idx][0].item())\n",
    "        color_id = int(meta_batch[sample_idx][1].item())\n",
    "        motion_score = float(meta_batch[sample_idx][2].item())\n",
    "        scene_name = [k for k, v in dataset.scene_to_id.items() if v == scene_id][0]\n",
    "        color_name = [k for k, v in dataset.color_to_id.items() if v == color_id][0]\n",
    "\n",
    "        print(f\"Scene: {scene_name}, Color: {color_name}, Motion: {motion_score:.3f}\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995cb404-b9ab-4552-aaad-992007054ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 EEG files → Total samples: 28000\n",
      "Loaded 1400 metadata JSON files\n",
      "Scene categories: 77 | Colors: 53\n",
      "Saved dataset to eeg_dataset.h5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# --- Paths ---\n",
    "HDF5_FILE = \"eeg_dataset.h5\"\n",
    "EEG_DIR = \"/home/poorna/data/preprocessed_eeg\"\n",
    "METADATA_DIR = \"/home/poorna/data/metadata_dir\"\n",
    "\n",
    "# --- Dataset ---\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/poorna/models/bert-base-uncased\")\n",
    "dataset = EEGTextMetaDataset(EEG_DIR, METADATA_DIR, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)  # batch=1 for sequential save\n",
    "\n",
    "# --- Create HDF5 file ---\n",
    "with h5py.File(HDF5_FILE, \"w\") as f:\n",
    "    n_samples = len(dataset)\n",
    "\n",
    "    # Create datasets (preallocate space)\n",
    "    eeg_shape = (n_samples, 62, 400)               # EEG shape\n",
    "    token_shape = (n_samples, dataset.max_length)  # tokenized input_ids length\n",
    "    meta_shape = (n_samples, 3)                    # scene_id, color_id, motion_score\n",
    "\n",
    "    eeg_ds = f.create_dataset(\"eeg\", shape=eeg_shape, dtype=\"float32\")\n",
    "    tokens_ds = f.create_dataset(\"input_ids\", shape=token_shape, dtype=\"int64\")\n",
    "    meta_ds = f.create_dataset(\"metadata\", shape=meta_shape, dtype=\"float32\")\n",
    "\n",
    "    # Iterate and save directly to file (low memory usage)\n",
    "    for idx, (eeg_tensor, tokenized, metadata_tensor) in enumerate(loader):\n",
    "        eeg_ds[idx] = eeg_tensor.squeeze(0).numpy()\n",
    "        tokens_ds[idx] = tokenized[\"input_ids\"].squeeze(0).numpy()\n",
    "        meta_ds[idx] = metadata_tensor.squeeze(0).numpy()\n",
    "\n",
    "print(f\"Saved dataset to {HDF5_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d9b072-b52a-40b0-8225-ee9336de4d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eeg', 'input_ids', 'metadata']\n",
      "torch.Size([62, 400]) torch.Size([64]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "with h5py.File(\"eeg_dataset.h5\", \"r\") as f:\n",
    "    print(list(f.keys()))  # ['eeg', 'input_ids', 'metadata']\n",
    "    eeg_sample = torch.tensor(f[\"eeg\"][0])         # first sample EEG tensor\n",
    "    tokens_sample = torch.tensor(f[\"input_ids\"][0])\n",
    "    meta_sample = torch.tensor(f[\"metadata\"][0])\n",
    "\n",
    "print(eeg_sample.shape, tokens_sample.shape, meta_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835b0bf3-689e-45c9-8a80-0fff0ccceda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets in file: ['eeg', 'input_ids', 'metadata']\n",
      "eeg: shape=(28000, 62, 400), dtype=float32\n",
      "input_ids: shape=(28000, 64), dtype=int64\n",
      "metadata: shape=(28000, 3), dtype=float32\n",
      "\n",
      "Sample check:\n",
      "EEG sample shape: (62, 400)\n",
      "Tokens sample shape: (64,)\n",
      "Metadata sample shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "HDF5_FILE = \"eeg_dataset.h5\"\n",
    "\n",
    "with h5py.File(HDF5_FILE, \"r\") as f:\n",
    "    print(\"\\nDatasets in file:\", list(f.keys()))\n",
    "\n",
    "    for name in f.keys():\n",
    "        dset = f[name]\n",
    "        print(f\"{name}: shape={dset.shape}, dtype={dset.dtype}\")\n",
    "\n",
    "    # Optional: verify a few entries\n",
    "    sample_idx = 0\n",
    "    print(\"\\nSample check:\")\n",
    "    print(\"EEG sample shape:\", f[\"eeg\"][sample_idx].shape)\n",
    "    print(\"Tokens sample shape:\", f[\"input_ids\"][sample_idx].shape)\n",
    "    print(\"Metadata sample shape:\", f[\"metadata\"][sample_idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a91a9-9514-47d2-ba1e-d699d0141008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
