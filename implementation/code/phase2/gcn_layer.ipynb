{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b220b6a1-3d33-4d92-8595-118d90aa6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phase 2 \n",
    "# step 1 defining the gcn layer\n",
    "#here W is the learnable weights, this is the layer that can adjust while learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d955d35-a0cf-4f5a-9ea8-33c341ec82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    #W can be created using nn.linear\n",
    "    #infeatures: number of features per channel (right now, 1 — the EEG signal’s raw amplitude at that time step).\n",
    "    #outfeatures: number of new features we want after mixing signals\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # Always call the parent class's constructor\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        # W matrix is represented by this linear layer\n",
    "        # It takes 'in_features' (e.g., 1 for raw signal)\n",
    "        # and transforms them into 'out_features' (e.g., 64 new features per channel)\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "        \n",
    "    def forward(self, x, adj_matrix):\n",
    "        #this is what the layer does on seeing the data \n",
    "        \n",
    "        # Input 'x' shape: (batch_size, n_channels, in_features) e.g., (32, 62, 1)\n",
    "        # Input 'adj_matrix' shape: (n_channels, n_channels) e.g., (62, 62)\n",
    "\n",
    "        # Step 1: Spatial Aggregation (A_hat * X)\n",
    "        # We use torch.bmm for batched matrix multiplication.\n",
    "        # adj_matrix needs to be unsqueezed or expanded to match batch_size\n",
    "        # The result will be (batch_size, n_channels, in_features)\n",
    "        \n",
    "        # Unsqueeze adj_matrix to (1, n_channels, n_channels) to enable broadcasting\n",
    "        # across the batch dimension when multiplying with x.\n",
    "        # Alternatively, you can explicitly expand it: adj_matrix.expand(x.size(0), -1, -1)\n",
    "        \n",
    "        # Note: In some GCN implementations, the order is X * W first, then A_hat * (XW).\n",
    "        # Your description implies A_hat * X first, then * W. Let's follow that.\n",
    "        \n",
    "        # First, multiply adj_matrix with the feature matrix 'x' for each item in the batch\n",
    "        # We need to make adj_matrix compatible with batch dimension for bmm\n",
    "        # If x is (B, N, F_in), adj_matrix is (N, N)\n",
    "        # We want (B, N, N) @ (B, N, F_in) -> (B, N, F_in)\n",
    "        \n",
    "        # This is a common way to handle it for batch processing with a shared A_hat:\n",
    "        aggregated_features = torch.bmm(adj_matrix.unsqueeze(0).expand(x.size(0), -1, -1), x)\n",
    "        \n",
    "        # Step 2: Feature Transformation ( (A_hat * X) * W )\n",
    "        # Apply the linear transformation (W matrix) to the aggregated features\n",
    "        # self.linear operates on the last dimension (in_features -> out_features)\n",
    "        transformed_features = self.linear(aggregated_features)\n",
    "        \n",
    "        # Step 3: Apply Activation Function\n",
    "        output = torch.relu(transformed_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed515038-e49d-4eae-8265-2af1532e9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing GCNLayer ---\n",
      "Loaded A_hat with shape: torch.Size([62, 62])\n",
      "GCNLayer instantiated with in_features=1, out_features=64\n",
      "Dummy input X shape: torch.Size([4, 62, 1])\n",
      "Output shape of GCNLayer: torch.Size([4, 62, 64])\n",
      "GCNLayer test passed: Output shape matches expected shape!\n",
      "--- GCNLayer Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Testing GCNLayer ---\")\n",
    "\n",
    "    # 1. Load your pre-computed A_hat (Adjacency Matrix)\n",
    "    # Make sure 'adj_matrix.pt' is in the same directory or provide its full path\n",
    "    try:\n",
    "        adj_matrix_path = \"/home/sanu/venvs/Ahat_adjacency_matrix.pt\" # Adjust this path!\n",
    "        A_hat = torch.load(adj_matrix_path)\n",
    "        print(f\"Loaded A_hat with shape: {A_hat.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Adjacency matrix file not found at {adj_matrix_path}.\")\n",
    "        print(\"Please ensure 'adj_matrix.pt' exists and the path is correct.\")\n",
    "        exit() # Exit if A_hat is not found\n",
    "\n",
    "    # Define input and output features for the GCN layer\n",
    "    in_features = 1  # Raw EEG signal has 1 feature per channel\n",
    "    out_features = 64 # Chosen output feature dimension (hyperparameter)\n",
    "\n",
    "    # 2. Instantiate the GCNLayer\n",
    "    gcn_layer = GCNLayer(in_features=in_features, out_features=out_features)\n",
    "    print(f\"GCNLayer instantiated with in_features={in_features}, out_features={out_features}\")\n",
    "\n",
    "    # 3. Create a dummy EEG input X\n",
    "    # Let's simulate a batch of 4 EEG samples, each with 62 channels and 1 feature per channel\n",
    "    batch_size = 4\n",
    "    n_channels = 62\n",
    "    dummy_x = torch.randn(batch_size, n_channels, in_features) # Random values for testing\n",
    "    print(f\"Dummy input X shape: {dummy_x.shape}\")\n",
    "\n",
    "    # 4. Pass dummy data through the GCNLayer\n",
    "    output = gcn_layer(dummy_x, A_hat)\n",
    "\n",
    "    # 5. Print the shape of the output\n",
    "    print(f\"Output shape of GCNLayer: {output.shape}\")\n",
    "\n",
    "    # Expected output shape should be (batch_size, n_channels, out_features)\n",
    "    # e.g., (4, 62, 64)\n",
    "    expected_shape = (batch_size, n_channels, out_features)\n",
    "    if output.shape == expected_shape:\n",
    "        print(\"GCNLayer test passed: Output shape matches expected shape!\")\n",
    "    else:\n",
    "        print(f\"GCNLayer test FAILED: Expected {expected_shape}, got {output.shape}\")\n",
    "\n",
    "    print(\"--- GCNLayer Test Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986b9a4-1cda-4eb3-867f-c165efc6f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Simulation ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Simulating GCNLayer processing in STGTEncoder ---\")\n",
    "\n",
    "    # 1. Load your all_eeg_tensors (from eeg_tensors.pt)\n",
    "    EEG_TENSORS_PATH = \"/home/sanu/venvs/capstone/implementation/code/phase2/eeg_tensors.pt\"\n",
    "    try:\n",
    "        all_eeg_data = torch.load(EEG_TENSORS_PATH)\n",
    "        print(f\"Loaded all_eeg_tensors from '{EEG_TENSORS_PATH}'. Shape: {all_eeg_data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: EEG tensors file not found at {EEG_TENSORS_PATH}.\")\n",
    "        print(\"Please ensure 'eeg_tensors.pt' exists at the specified path.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading EEG tensors: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Load your pre-computed A_hat (adjacency matrix)\n",
    "    ADJ_MATRIX_PATH = \"/home/sanu/Projects/Capstone_EEG/adj_matrix.pt\" # Adjust this path!\n",
    "    try:\n",
    "        A_hat = torch.load(ADJ_MATRIX_PATH)\n",
    "        print(f\"Loaded A_hat with shape: {A_hat.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Adjacency matrix file not found at {ADJ_MATRIX_PATH}.\")\n",
    "        print(\"Please ensure 'adj_matrix.pt' exists at the specified path.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading A_hat: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Define GCNLayer parameters\n",
    "    in_features = 1       # Raw EEG signal has 1 feature per channel\n",
    "    out_features = 64     # Output feature dimension of your GCNLayer (e.g., 64)\n",
    "    n_channels = 62       # Number of EEG channels\n",
    "    n_time_steps = 400    # Number of time steps in your EEG data\n",
    "\n",
    "    # Instantiate your GCNLayer\n",
    "    gcn_layer = GCNLayer(in_features=in_features, out_features=out_features)\n",
    "    print(f\"\\nInstantiated GCNLayer with in_features={in_features}, out_features={out_features}\")\n",
    "\n",
    "    # --- Simulate a batch from a DataLoader ---\n",
    "    # For demonstration, let's take a small batch of 4 samples from your all_eeg_data\n",
    "    batch_size = 4\n",
    "    if all_eeg_data.shape[0] < batch_size:\n",
    "        print(f\"Warning: Not enough samples ({all_eeg_data.shape[0]}) for batch size {batch_size}. Using all available samples.\")\n",
    "        batch_size = all_eeg_data.shape[0]\n",
    "\n",
    "    # Select a batch of data (e.g., first 'batch_size' samples)\n",
    "    eeg_batch_data = all_eeg_data[:batch_size, :, :] # Shape: (batch_size, 62, 400)\n",
    "    print(f\"Simulating a batch of EEG data with shape: {eeg_batch_data.shape}\")\n",
    "\n",
    "    # This list will store the output of the GCN for each time step\n",
    "    # for all samples in the batch.\n",
    "    gcn_outputs_per_timestep = []\n",
    "\n",
    "    # --- Loop through each time step to process with GCNLayer ---\n",
    "    print(\"\\nProcessing each time step with GCNLayer...\")\n",
    "    for t in range(n_time_steps):\n",
    "        # 1. Extract X for the current time step for ALL samples in the batch\n",
    "        # This slice: eeg_batch_data[:, :, t] gives a tensor of shape (batch_size, 62)\n",
    "        X_for_current_timestep_batch = eeg_batch_data[:, :, t]\n",
    "\n",
    "        # 2. Reshape X to (batch_size, 62, 1) for the GCNLayer's input\n",
    "        # The '1' is the 'in_features' dimension.\n",
    "        X_for_gcn_input = X_for_current_timestep_batch.unsqueeze(2) \n",
    "        # print(f\"  Time step {t}: X_for_gcn_input shape: {X_for_gcn_input.shape}\") # Uncomment for verbose debugging\n",
    "\n",
    "        # 3. Pass X and A_hat through the GCNLayer\n",
    "        # Output will be (batch_size, n_channels, out_features)\n",
    "        gcn_output_t = gcn_layer(X_for_gcn_input, A_hat)\n",
    "        \n",
    "        # 4. Store the output\n",
    "        gcn_outputs_per_timestep.append(gcn_output_t)\n",
    "\n",
    "    # --- After processing all time steps ---\n",
    "    # Stack the list of outputs into a single tensor\n",
    "    # gcn_outputs_per_timestep is a list of N_TIME_STEPS tensors, each (batch_size, 62, out_features)\n",
    "    # torch.stack will combine them along a new dimension (dimension 0 by default, or specified)\n",
    "    # We want (batch_size, N_TIME_STEPS, 62, out_features)\n",
    "    final_gcn_sequence_output = torch.stack(gcn_outputs_per_timestep, dim=1)\n",
    "    \n",
    "    print(\"\\nFinished processing all time steps with GCNLayer.\")\n",
    "    print(f\"Shape of the final GCN sequence output: {final_gcn_sequence_output.shape}\")\n",
    "\n",
    "    # Expected output shape: (batch_size, n_time_steps, n_channels, out_features)\n",
    "    expected_final_shape = (batch_size, n_time_steps, n_channels, out_features)\n",
    "    if final_gcn_sequence_output.shape == expected_final_shape:\n",
    "        print(\"Simulation successful: Final GCN sequence output shape matches expected!\")\n",
    "    else:\n",
    "        print(f\"Simulation FAILED: Expected {expected_final_shape}, got {final_gcn_sequence_output.shape}\")\n",
    "\n",
    "    print(\"\\nThis `final_gcn_sequence_output` is what will typically be fed into your Temporal Transformer.\")\n",
    "    print(\"The next step in building the STGT Encoder is to integrate this into a full class.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone venv",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
