{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b048ba3-73c2-4be0-8dfd-581f0dbcf00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing GCNLayer ---\n",
      "Loaded A_hat with shape: torch.Size([62, 62])\n",
      "GCNLayer instantiated with in_features=1, out_features=64\n",
      "Dummy input X shape: torch.Size([4, 62, 1])\n",
      "Output shape of GCNLayer: torch.Size([4, 62, 64])\n",
      "GCNLayer test passed: Output shape matches expected shape!\n",
      "--- GCNLayer Test Complete ---\n",
      "--- Simulating GCNLayer processing in STGTEncoder ---\n",
      "Loaded all_eeg_tensors from './eeg_tensors.pt'. Shape: torch.Size([28000, 62, 400])\n",
      "Loaded A_hat with shape: torch.Size([62, 62])\n",
      "\n",
      "Instantiated GCNLayer with in_features=1, out_features=64\n",
      "Simulating a batch of EEG data with shape: torch.Size([4, 62, 400])\n",
      "\n",
      "Processing each time step with GCNLayer...\n",
      "\n",
      "Finished processing all time steps with GCNLayer.\n",
      "Shape of the final GCN sequence output: torch.Size([4, 400, 62, 64])\n",
      "Simulation successful: Final GCN sequence output shape matches expected!\n",
      "\n",
      "This `final_gcn_sequence_output` is what will typically be fed into your Temporal Transformer.\n",
      "The next step in building the STGT Encoder is to integrate this into a full class.\n",
      "--- Testing TemporalTransformer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poorna/venvs/torch/lib64/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalTransformer instantiated with d_model=3968, nhead=8, num_layers=3\n",
      "Dummy input sequence shape: torch.Size([400, 4, 3968])\n",
      "Output sequence shape of TemporalTransformer: torch.Size([400, 4, 3968])\n",
      "TemporalTransformer test passed: Output shape matches expected shape!\n",
      "--- TemporalTransformer Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTANT: Run these cells first in your Jupyter Notebook ---\n",
    "# These commands will execute the entire gcn_layer.ipynb and temporal_transformer.ipynb files.\n",
    "# This makes the classes (GCNLayer, TemporalTransformer) and variables (like TRANSFORMER_D_MODEL)\n",
    "# defined in those notebooks available in this current notebook's global scope.\n",
    "%run gcn_layer.ipynb\n",
    "%run temporal_transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edd6d7-572f-4246-beca-8b3be64814fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify that the classes and variables are now accessible ---\n",
    "# You can uncomment these lines to check if the imports worked after running the above.\n",
    "# print(f\"GCNLayer class available: {GCNLayer is not None}\")\n",
    "# print(f\"TemporalTransformer class available: {TemporalTransformer is not None}\")\n",
    "# print(f\"TRANSFORMER_D_MODEL available: {TRANSFORMER_D_MODEL}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Define the STGTEncoder Class ---\n",
    "# This class combines the GCNLayer and TemporalTransformer for spatio-temporal EEG processing.\n",
    "class STGTEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_time_steps, gcn_out_features, \n",
    "                 transformer_d_model, transformer_nhead, transformer_num_layers, \n",
    "                 transformer_dim_feedforward, transformer_dropout=0.1):\n",
    "        super(STGTEncoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels # e.g., 62 for EEG channels\n",
    "        self.num_time_steps = num_time_steps # e.g., 400 for EEG samples per trial\n",
    "\n",
    "        # --- GCN Layer (Spatial Processing) ---\n",
    "        # Takes 1 input feature (raw signal amplitude) and outputs 'gcn_out_features'\n",
    "        self.gcn_layer = GCNLayer(in_features=1, out_features=gcn_out_features)\n",
    "\n",
    "        # --- Validate and Set Transformer d_model ---\n",
    "        # The transformer_d_model must be the flattened size of (in_channels * gcn_out_features)\n",
    "        # as this will be the input dimension for each time step's token in the Transformer.\n",
    "        expected_transformer_d_model = in_channels * gcn_out_features\n",
    "        if transformer_d_model != expected_transformer_d_model:\n",
    "            raise ValueError(\n",
    "                f\"transformer_d_model ({transformer_d_model}) must be equal to \"\n",
    "                f\"in_channels ({in_channels}) * gcn_out_features ({gcn_out_features}) \"\n",
    "                f\"= {expected_transformer_d_model}. Please check your hyperparameters.\"\n",
    "            )\n",
    "        self.transformer_d_model = transformer_d_model\n",
    "\n",
    "        # --- Temporal Transformer (Temporal Processing) ---\n",
    "        self.temporal_transformer = TemporalTransformer(\n",
    "            d_model=self.transformer_d_model,\n",
    "            nhead=transformer_nhead,\n",
    "            num_encoder_layers=transformer_num_layers,\n",
    "            dim_feedforward=transformer_dim_feedforward,\n",
    "            dropout=transformer_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, eeg_batch_data, adj_matrix):\n",
    "        # eeg_batch_data shape: (batch_size, in_channels, num_time_steps) e.g., (32, 62, 400)\n",
    "        # adj_matrix shape: (in_channels, in_channels) e.g., (62, 62)\n",
    "\n",
    "        batch_size = eeg_batch_data.size(0)\n",
    "        \n",
    "        # List to store the output features from the GCN for each time step\n",
    "        gcn_outputs_sequence = []\n",
    "\n",
    "        # --- Step 1: Spatial Processing with GCN for each Time Step ---\n",
    "        for t in range(self.num_time_steps):\n",
    "            # Extract X for the current time step for ALL samples in the batch\n",
    "            # Shape: (batch_size, in_channels)\n",
    "            X_for_current_timestep_batch = eeg_batch_data[:, :, t]\n",
    "\n",
    "            # Reshape X to (batch_size, in_channels, 1) to match GCNLayer's in_features=1\n",
    "            X_for_gcn_input = X_for_current_timestep_batch.unsqueeze(2) \n",
    "\n",
    "            # Pass X and A_hat through the GCNLayer\n",
    "            # Output will be (batch_size, in_channels, gcn_out_features)\n",
    "            gcn_output_t = self.gcn_layer(X_for_gcn_input, adj_matrix)\n",
    "            \n",
    "            # Store the GCN output for this time step\n",
    "            gcn_outputs_sequence.append(gcn_output_t)\n",
    "\n",
    "        # --- Step 2: Prepare GCN Outputs for Temporal Transformer ---\n",
    "        # Stack the list of GCN outputs into a single tensor.\n",
    "        # This creates a tensor of shape (batch_size, num_time_steps, in_channels, gcn_out_features).\n",
    "        stacked_gcn_output = torch.stack(gcn_outputs_sequence, dim=1)\n",
    "        \n",
    "        # Flatten the (in_channels, gcn_out_features) part into a single dimension.\n",
    "        # This converts each time step's (channels x features) into a single \"token\" vector.\n",
    "        # Shape: (batch_size, num_time_steps, transformer_d_model)\n",
    "        transformer_input_flat = stacked_gcn_output.view(\n",
    "            batch_size, self.num_time_steps, self.transformer_d_model\n",
    "        )\n",
    "\n",
    "        # Permute dimensions to (num_time_steps, batch_size, transformer_d_model).\n",
    "        # This is the standard input order expected by PyTorch's TransformerEncoder\n",
    "        # when its 'batch_first' parameter is set to False (as it is in TemporalTransformer).\n",
    "        transformer_input_permuted = transformer_input_flat.permute(1, 0, 2)\n",
    "        \n",
    "        # --- Step 3: Temporal Processing with Transformer ---\n",
    "        # The Transformer processes the sequence of spatially-enriched tokens.\n",
    "        # Output will have the same shape: (num_time_steps, batch_size, transformer_d_model).\n",
    "        transformer_output_sequence = self.temporal_transformer(transformer_input_permuted)\n",
    "        \n",
    "        # --- Step 4: Aggregate Final Embedding ---\n",
    "        # To get a single, fixed-size embedding for the entire EEG trial,\n",
    "        # we average the Transformer's output sequence across the time steps.\n",
    "        # The result is an embedding of shape (batch_size, transformer_d_model).\n",
    "        final_eeg_embedding = transformer_output_sequence.mean(dim=0)\n",
    "        \n",
    "        return final_eeg_embedding\n",
    "\n",
    "# --- Test and Verify the Full STGTEncoder ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Testing STGTEncoder ---\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    # Adjust these paths to where your files are located\n",
    "    EEG_TENSORS_PATH = \"./eeg_tensors.pt\" \n",
    "    ADJ_MATRIX_PATH = \"/home/sanu/Projects/Capstone_EEG/adj_matrix.pt\" \n",
    "\n",
    "    try:\n",
    "        all_eeg_data = torch.load(EEG_TENSORS_PATH)\n",
    "        print(f\"Loaded all_eeg_tensors. Shape: {all_eeg_data.shape}\")\n",
    "        A_hat = torch.load(ADJ_MATRIX_PATH)\n",
    "        print(f\"Loaded A_hat. Shape: {A_hat.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading required files: {e}\")\n",
    "        print(\"Please ensure 'eeg_tensors.pt' and 'adj_matrix.pt' exist at the specified paths.\")\n",
    "        # If running in a notebook, you might remove exit() to continue with dummy data for testing\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Define Hyperparameters for STGTEncoder ---\n",
    "    in_channels = 62            # Number of EEG channels\n",
    "    num_time_steps = 400        # Number of time steps in each EEG trial\n",
    "    gcn_out_features = 64       # Number of output features from the GCNLayer per channel\n",
    "\n",
    "    # Transformer's d_model must match (in_channels * gcn_out_features) for the flattening\n",
    "    transformer_d_model = in_channels * gcn_out_features # 62 * 64 = 3968\n",
    "    transformer_nhead = 8\n",
    "    transformer_num_layers = 3\n",
    "    transformer_dim_feedforward = 4 * transformer_d_model\n",
    "    transformer_dropout = 0.1\n",
    "\n",
    "    # --- 3. Instantiate STGTEncoder ---\n",
    "    stgt_encoder = STGTEncoder(\n",
    "        in_channels=in_channels,\n",
    "        num_time_steps=num_time_steps,\n",
    "        gcn_out_features=gcn_out_features,\n",
    "        transformer_d_model=transformer_d_model,\n",
    "        transformer_nhead=transformer_nhead,\n",
    "        transformer_num_layers=transformer_num_layers,\n",
    "        transformer_dim_feedforward=transformer_dim_feedforward,\n",
    "        transformer_dropout=transformer_dropout\n",
    "    )\n",
    "    print(f\"\\nSTGTEncoder instantiated with gcn_out_features={gcn_out_features} and transformer_d_model={transformer_d_model}\")\n",
    "\n",
    "    # --- 4. Prepare Dummy Batch from Loaded Data ---\n",
    "    batch_size = 4\n",
    "    if all_eeg_data.shape[0] < batch_size:\n",
    "        print(f\"Warning: Not enough samples ({all_eeg_data.shape[0]}) for batch size {batch_size}. Using all available samples.\")\n",
    "        batch_size = all_eeg_data.shape[0]\n",
    "\n",
    "    # Select a batch of EEG data (e.g., first 'batch_size' samples)\n",
    "    dummy_eeg_batch = all_eeg_data[:batch_size, :, :]\n",
    "    print(f\"Dummy EEG batch shape: {dummy_eeg_batch.shape}\")\n",
    "\n",
    "    # --- 5. Pass through STGTEncoder ---\n",
    "    print(\"\\nRunning STGTEncoder forward pass...\")\n",
    "    final_embedding = stgt_encoder(dummy_eeg_batch, A_hat)\n",
    "\n",
    "    # --- 6. Print Output Shape ---\n",
    "    print(f\"Shape of the final EEG embedding from STGTEncoder: {final_embedding.shape}\")\n",
    "\n",
    "    # Expected output shape: (batch_size, transformer_d_model)\n",
    "    expected_output_shape = (batch_size, transformer_d_model)\n",
    "    if final_embedding.shape == expected_output_shape:\n",
    "        print(\"STGTEncoder test passed: Output embedding shape matches expected shape! ✅\")\n",
    "    else:\n",
    "        print(f\"STGTEncoder test FAILED: Expected {expected_output_shape}, got {final_embedding.shape} ❌\")\n",
    "\n",
    "    print(\"\\n--- STGTEncoder Test Complete ---\")\n",
    "    print(\"This final_embedding is the rich EEG representation ready for the next phases: \"\n",
    "          \"Contrastive Alignment, MTR Head, and Text Decoder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0fb56-b95d-4116-b747-48978cadb9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
